{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Exploration and Preparation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll analyze HR data from Salifort Motors to build a predictive model for employee turnover. This analysis will help the company identify which employees are most likely to leave and understand the key factors contributing to turnover.\n",
    "\n",
    "Our analysis is divided into three main parts:\n",
    "1. Data Exploration and Preparation\n",
    "2. Visualization and Feature Engineering\n",
    "3. Model Building and Evaluation\n",
    "\n",
    "Let's start by importing the necessary libraries and loading the data.\n",
    "\n",
    "## 1.1 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Centralized imports and utility functions for HR Turnover Prediction project.\n",
    "\"\"\"\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from typing import (\n",
    "    Dict, \n",
    "    List, \n",
    "    Tuple, \n",
    "    Optional, \n",
    "    Union, \n",
    "    Callable\n",
    ")\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning Core\n",
    "from sklearn.base import (\n",
    "    BaseEstimator, \n",
    "    TransformerMixin, \n",
    "    ClassifierMixin\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    learning_curve  \n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    SelectFromModel\n",
    ")\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    Lasso\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Scientific Computing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Ignore warnings (optional)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Verify required libraries are installed.\"\"\"\n",
    "    required_libs = [\n",
    "        'numpy', 'pandas', 'sklearn', \n",
    "        'xgboost', 'matplotlib', 'seaborn'\n",
    "    ]\n",
    "    for lib in required_libs:\n",
    "        try:\n",
    "            __import__(lib)\n",
    "            logger.info(f\"{lib} is installed ✅\")\n",
    "        except ImportError:\n",
    "            logger.warning(f\"{lib} is NOT installed ❌\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_dependencies()\n",
    "\n",
    "# For visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Set figure size and resolution for all plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a directory for plots\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Data\n",
    "\n",
    "Next, we'll load the dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "hr_data = pd.read_csv(\"HR_capstone_dataset.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "hr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explore Dataset Information\n",
    "\n",
    "Let's examine the structure of our dataset, including data types and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset info:\")\n",
    "hr_data.info()\n",
    "\n",
    "print(\"\\nDataset dimensions:\", hr_data.shape)\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(hr_data.columns.tolist())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics for numerical columns:\")\n",
    "hr_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary for categorical columns\n",
    "print(\"Summary of categorical columns:\")\n",
    "hr_data.describe(include=['object'])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = hr_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(hr_data)) * 100\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Count of unique values in each column\n",
    "print(\"\\nCount of unique values in each column:\")\n",
    "for col in hr_data.columns:\n",
    "    print(f\"{col}: {hr_data[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Cleaning\n",
    "\n",
    "Now let's clean the data by:\n",
    "1. Renaming columns for better clarity\n",
    "2. Checking for and removing duplicate entries\n",
    "3. Ensuring consistent data formats\n",
    "\n",
    "This will prepare our data for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Rename columns for better readability\n",
    "    column_rename = {\n",
    "        'satisfaction_level': 'satisfaction_level',\n",
    "        'last_evaluation': 'last_evaluation',\n",
    "        'number_project': 'num_projects',\n",
    "        'average_montly_hours': 'monthly_hours',  # Fix typo in original column name\n",
    "        'time_spend_company': 'tenure',\n",
    "        'Work_accident': 'had_accident',\n",
    "        'left': 'left_company',\n",
    "        'promotion_last_5years': 'promotion_last_5yr',\n",
    "        'Department': 'department',  # Make lowercase for consistency\n",
    "        'salary': 'salary_level'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_rename)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        df = df.drop_duplicates(keep='first')\n",
    "        print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "hr_data_clean = clean_data(hr_data)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "hr_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Visualization and Feature Engineering\n",
    "\n",
    "In this section, we'll:\n",
    "1. Create meaningful features to better capture employee behavior and turnover risk\n",
    "2. Visualize relationships between key variables and employee turnover\n",
    "3. Identify patterns that could help us understand why employees leave\n",
    "\n",
    "## 2.1 Feature Engineering\n",
    "\n",
    "First, let's create new features that might help us better predict turnover. These include:\n",
    "- Work intensity metrics\n",
    "- Workload indicators\n",
    "- Risk factors based on combinations of variables\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create new features that might help predict employee turnover\"\"\"\n",
    "    \n",
    "    # Create a feature for work intensity (monthly hours / projects)\n",
    "    # This measures how time-intensive each project is for an employee\n",
    "    df['work_intensity'] = df['monthly_hours'] / df['num_projects']\n",
    "    \n",
    "    # Create a feature for hours relative to average (how much someone works relative to average)\n",
    "    # This normalizes hours worked against the company average to identify outliers\n",
    "    avg_hours = df['monthly_hours'].mean()\n",
    "    df['relative_hours'] = df['monthly_hours'] / avg_hours\n",
    "    \n",
    "    # Create a feature for overworked employees (above average hours and projects)\n",
    "    # This identifies employees with both high hours and high project load\n",
    "    # Using 4 projects as threshold based on the median being 4 projects\n",
    "    df['overworked'] = ((df['monthly_hours'] > avg_hours) & (df['num_projects'] > 4)).astype(int)\n",
    "    \n",
    "    # Create work-life balance indicator (if monthly hours are excessive)\n",
    "    # 200 hours represents 25% more than the standard 160 hours per month\n",
    "    # Assuming 160 hours is standard (8 hours × 20 days)\n",
    "    df['poor_work_life_balance'] = (df['monthly_hours'] > 200).astype(int)\n",
    "    \n",
    "    # Create a feature for productivity (evaluation score / projects)\n",
    "    # This measures performance efficiency per project\n",
    "    df['productivity'] = df['last_evaluation'] / df['num_projects']\n",
    "    \n",
    "    # Create a feature for satisfaction-to-workload ratio\n",
    "    # Normalized to standard 160-hour month to make interpretation easier\n",
    "    df['satisfaction_per_hour'] = df['satisfaction_level'] / (df['monthly_hours'] / 160)\n",
    "    \n",
    "    # Create a feature for under-utilized employees (high evaluation but few projects)\n",
    "    # 0.75 represents top quartile of evaluations with below-average project count\n",
    "    df['underutilized'] = ((df['last_evaluation'] > 0.75) & (df['num_projects'] < 3)).astype(int)\n",
    "    \n",
    "    # Create a feature for \"burnout risk\" employees\n",
    "    # 250 hours represents ~60+ hours per week, a clear burnout indicator\n",
    "    # 5+ projects is above the 75th percentile\n",
    "    # Satisfaction below 0.5 indicates dissatisfaction\n",
    "    df['burnout_risk'] = ((df['monthly_hours'] > 250) & \n",
    "                          (df['num_projects'] > 5) & \n",
    "                          (df['satisfaction_level'] < 0.5)).astype(int)\n",
    "    \n",
    "    # Create a feature for flight risk based on satisfaction, evaluation, and tenure\n",
    "    # Low satisfaction (<0.4) but high performance (>0.7) typically indicates \n",
    "    # talented employees who may be disengaged\n",
    "    # Tenure > 3 years focuses on experienced employees who may be plateauing\n",
    "    df['flight_risk'] = ((df['satisfaction_level'] < 0.4) & \n",
    "                         (df['tenure'] > 3) & \n",
    "                         (df['last_evaluation'] > 0.7)).astype(int)\n",
    "    \n",
    "    # Create a feature for high performers who might be flight risks\n",
    "    # Evaluation > 0.8 represents top performers (highest quintile)\n",
    "    # No promotion in 5 years despite long tenure suggests career stagnation\n",
    "    # Tenure > 4 years focuses on employees who have been with the company long enough\n",
    "    # to expect career advancement\n",
    "    df['high_performer_risk'] = ((df['last_evaluation'] > 0.8) & \n",
    "                                (df['promotion_last_5yr'] == 0) & \n",
    "                                (df['tenure'] > 4)).astype(int)\n",
    "                                \n",
    "    return df\n",
    "\n",
    "# Engineer features\n",
    "hr_data_engineered = engineer_features(hr_data_clean)\n",
    "\n",
    "# Display the first few rows of the engineered dataset with the new features\n",
    "hr_data_engineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the summary statistics of our engineered features\n",
    "hr_data_engineered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Visualization\n",
    "\n",
    "Now let's create visualizations to understand patterns in employee turnover.\n",
    "\n",
    "### 2.2.1 Overall Turnover Distribution\n",
    "\n",
    "First, let's look at the overall turnover rate in the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee turnover distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "turnover_counts = hr_data_engineered['left_company'].value_counts()\n",
    "turnover_percent = hr_data_engineered['left_company'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the percentages\n",
    "print(\"Turnover distribution:\")\n",
    "print(f\"Stayed: {turnover_percent[0]:.1f}%\")\n",
    "print(f\"Left: {turnover_percent[1]:.1f}%\")\n",
    "\n",
    "# Create pie chart\n",
    "plt.pie(turnover_counts, labels=['Stayed', 'Left'] if turnover_counts.index[0] == 0 else ['Left', 'Stayed'],\n",
    "        autopct='%1.1f%%', startangle=90, colors=['#66b3ff', '#ff9999'],\n",
    "        wedgeprops={'edgecolor': 'w', 'linewidth': 1})\n",
    "\n",
    "plt.title('Employee Turnover Distribution', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/turnover_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Satisfaction Level and Evaluation Score\n",
    "\n",
    "Let's explore how satisfaction levels and evaluation scores relate to turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Satisfaction level by turnover status\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='left_company', y='satisfaction_level', data=hr_data_engineered, inner='box', palette='Set2')\n",
    "plt.title('Employee Satisfaction Level by Turnover Status', fontsize=16)\n",
    "plt.xlabel('Left Company (0=No, 1=Yes)', fontsize=14)\n",
    "plt.ylabel('Satisfaction Level', fontsize=14)\n",
    "plt.xticks([0, 1], ['Stayed', 'Left'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig('plots/satisfaction_by_turnover.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Last evaluation score by turnover status\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='left_company', y='last_evaluation', data=hr_data_engineered, inner='box', palette='Set2')\n",
    "plt.title('Last Evaluation Score by Turnover Status', fontsize=16)\n",
    "plt.xlabel('Left Company (0=No, 1=Yes)', fontsize=14)\n",
    "plt.ylabel('Evaluation Score', fontsize=14)\n",
    "plt.xticks([0, 1], ['Stayed', 'Left'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig('plots/evaluation_by_turnover.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Satisfaction vs. Evaluation\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = sns.scatterplot(data=hr_data_engineered, x='satisfaction_level', y='last_evaluation', \n",
    "                hue='left_company', palette=['#66b3ff', '#ff9999'], \n",
    "                alpha=0.6, s=100)\n",
    "plt.title('Satisfaction Level vs. Evaluation Score by Turnover Status', fontsize=16)\n",
    "plt.xlabel('Satisfaction Level', fontsize=14)\n",
    "plt.ylabel('Last Evaluation Score', fontsize=14)\n",
    "plt.legend(title='Employee Status', labels=['Stayed', 'Left'], fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/satisfaction_vs_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Workload Analysis\n",
    "\n",
    "Let's explore the relationship between workload (number of projects and monthly hours) and turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Number of projects by turnover status\n",
    "plt.figure(figsize=(14, 8))\n",
    "project_turnover = pd.crosstab(hr_data_engineered['num_projects'], hr_data_engineered['left_company'])\n",
    "project_turnover_pct = project_turnover.div(project_turnover.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plot count\n",
    "plt.subplot(1, 2, 1)\n",
    "# Convert left_company to string type if needed\n",
    "hr_data_engineered['left_company_str'] = hr_data_engineered['left_company'].astype(str)\n",
    "sns.countplot(data=hr_data_engineered, x='num_projects', hue='left_company_str', palette='Set2')\n",
    "plt.title('Number of Projects by Turnover Status (Count)', fontsize=14)\n",
    "plt.xlabel('Number of Projects', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.legend(title='Employee Status', labels=['0 (Stayed)', '1 (Left)'])\n",
    "\n",
    "# Plot percentage\n",
    "plt.subplot(1, 2, 2)\n",
    "project_turnover_pct[1].plot(kind='bar', color='#ff9999', figsize=(14, 8))\n",
    "plt.title('Turnover Rate by Number of Projects', fontsize=14)\n",
    "plt.xlabel('Number of Projects', fontsize=12)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/projects_by_turnover.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Monthly hours distribution by turnover status\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.histplot(data=hr_data_engineered, x='monthly_hours', hue='left_company', bins=30, element='step', \n",
    "             palette='Set2', common_norm=False)\n",
    "plt.axvline(x=160, color='red', linestyle='--', label='Standard 160 hours/month')\n",
    "plt.title('Distribution of Monthly Hours by Turnover Status', fontsize=16)\n",
    "plt.xlabel('Average Monthly Hours', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(title='Employee Status', labels=['Stayed', 'Left', 'Standard 160 hours/month'])\n",
    "plt.savefig('plots/monthly_hours_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Monthly Hours vs Number of Projects\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = sns.scatterplot(data=hr_data_engineered, x='monthly_hours', y='num_projects', \n",
    "                hue='left_company', palette=['#66b3ff', '#ff9999'], \n",
    "                alpha=0.6, s=100)\n",
    "plt.axvline(x=160, color='red', linestyle='--', label='Standard 160 hours/month')\n",
    "plt.title('Monthly Hours vs. Number of Projects by Turnover Status', fontsize=16)\n",
    "plt.xlabel('Monthly Hours', fontsize=14)\n",
    "plt.ylabel('Number of Projects', fontsize=14)\n",
    "plt.legend(title='Employee Status', labels=['Stayed', 'Left', 'Standard Hours'], fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/hours_vs_projects.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Tenure and Department Analysis\n",
    "\n",
    "Let's examine how time spent at the company and department relate to turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tenure distribution by turnover status\n",
    "plt.figure(figsize=(14, 8))\n",
    "tenure_turnover = pd.crosstab(hr_data_engineered['tenure'], hr_data_engineered['left_company'])\n",
    "tenure_turnover_pct = tenure_turnover.div(tenure_turnover.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Create a mapped column for meaningful labels\n",
    "hr_data_engineered['turnover_status'] = hr_data_engineered['left_company'].map({0: 'Stayed', 1: 'Left'})\n",
    "\n",
    "# Plot count\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=hr_data_engineered, x='tenure', hue='turnover_status', palette='Set2')\n",
    "plt.title('Years at Company by Turnover Status (Count)', fontsize=14)\n",
    "plt.xlabel('Years at Company', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.legend(title='Employee Status')\n",
    "\n",
    "# Plot percentage\n",
    "plt.subplot(1, 2, 2)\n",
    "tenure_turnover_pct[1].plot(kind='bar', color='#ff9999')\n",
    "plt.title('Turnover Rate by Years at Company', fontsize=14)\n",
    "plt.xlabel('Years at Company', fontsize=12)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/tenure_by_turnover.png')\n",
    "plt.show()\n",
    "# 2. Turnover rate by department\n",
    "plt.figure(figsize=(14, 8))\n",
    "dept_turnover = pd.crosstab(hr_data_engineered['department'], hr_data_engineered['left_company'])\n",
    "dept_turnover['total'] = dept_turnover.sum(axis=1)\n",
    "dept_turnover['turnover_rate'] = (dept_turnover[1] / dept_turnover['total'] * 100).round(2)\n",
    "dept_turnover = dept_turnover.sort_values('turnover_rate', ascending=False)\n",
    "\n",
    "# Plot the turnover rate by department\n",
    "turnover_rates = dept_turnover['turnover_rate']\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_plot = sns.barplot(x=turnover_rates.index, y=turnover_rates.values, palette='Set2')\n",
    "plt.title('Turnover Rate by Department', fontsize=16)\n",
    "plt.xlabel('Department', fontsize=14)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(turnover_rates.values):\n",
    "    bar_plot.text(i, v + 0.5, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/turnover_by_department.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Salary and Risk Factor Analysis\n",
    "\n",
    "Finally, let's examine how salary and our engineered risk factors relate to turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Turnover rate by salary level\n",
    "plt.figure(figsize=(12, 8))\n",
    "salary_turnover = pd.crosstab(hr_data_engineered['salary_level'], hr_data_engineered['left_company'])\n",
    "salary_turnover['total'] = salary_turnover.sum(axis=1)\n",
    "salary_turnover['turnover_rate'] = (salary_turnover[1] / salary_turnover['total'] * 100).round(2)\n",
    "\n",
    "# Order by salary level (low, medium, high)\n",
    "salary_order = ['low', 'medium', 'high']\n",
    "salary_turnover = salary_turnover.reindex(salary_order)\n",
    "\n",
    "# Plot the turnover rate by salary level\n",
    "turnover_rates = salary_turnover['turnover_rate']\n",
    "bar_plot = sns.barplot(x=turnover_rates.index, y=turnover_rates.values, palette='Set2')\n",
    "plt.title('Turnover Rate by Salary Level', fontsize=16)\n",
    "plt.xlabel('Salary Level', fontsize=14)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(turnover_rates.values):\n",
    "    bar_plot.text(i, v + 0.5, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/turnover_by_salary.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Risk Factors Analysis\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Work intensity by turnover\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='left_company', y='work_intensity', data=hr_data_engineered, palette='Set2')\n",
    "plt.title('Work Intensity by Turnover', fontsize=14)\n",
    "plt.xlabel('Left Company', fontsize=12)\n",
    "plt.xticks([0, 1], ['Stayed', 'Left'], fontsize=10)\n",
    "\n",
    "# Work-life balance by turnover\n",
    "plt.subplot(2, 2, 2)\n",
    "balance_count = pd.crosstab(hr_data_engineered['poor_work_life_balance'], hr_data_engineered['left_company'])\n",
    "balance_pct = balance_count.div(balance_count.sum(axis=1), axis=0) * 100\n",
    "balance_pct[1].plot(kind='bar', color='#ff9999')\n",
    "plt.title('Turnover Rate by Poor Work-Life Balance', fontsize=14)\n",
    "plt.xlabel('Poor Work-Life Balance (0=No, 1=Yes)', fontsize=12)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Burnout risk by turnover\n",
    "plt.subplot(2, 2, 3)\n",
    "burnout_count = pd.crosstab(hr_data_engineered['burnout_risk'], hr_data_engineered['left_company'])\n",
    "burnout_pct = burnout_count.div(burnout_count.sum(axis=1), axis=0) * 100\n",
    "burnout_pct[1].plot(kind='bar', color='#ff9999')\n",
    "plt.title('Turnover Rate by Burnout Risk', fontsize=14)\n",
    "plt.xlabel('Burnout Risk (0=No, 1=Yes)', fontsize=12)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# High performer risk by turnover\n",
    "plt.subplot(2, 2, 4)\n",
    "performer_count = pd.crosstab(hr_data_engineered['high_performer_risk'], hr_data_engineered['left_company'])\n",
    "performer_pct = performer_count.div(performer_count.sum(axis=1), axis=0) * 100\n",
    "performer_pct[1].plot(kind='bar', color='#ff9999')\n",
    "plt.title('Turnover Rate by High Performer Risk', fontsize=14)\n",
    "plt.xlabel('High Performer Risk (0=No, 1=Yes)', fontsize=12)\n",
    "plt.ylabel('Turnover Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/engineered_features.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "numeric_cols = hr_data_engineered.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation = hr_data_engineered[numeric_cols].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(correlation)\n",
    "\n",
    "heatmap = sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                      mask=mask, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Building and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Preparation for Modeling\n",
    "\n",
    "#### Now that we've explored and engineered our data, let's prepare it for modeling by:\n",
    " - Splitting features and target variable\n",
    " - Creating a preprocessing pipeline for numerical and categorical features\n",
    " - Splitting the data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data_preparation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def prepare_data_for_modeling(\n",
    "    df, \n",
    "    target_column='left_company', \n",
    "    categorical_features=None, \n",
    "    numerical_features=None,\n",
    "    test_size=0.2,\n",
    "    val_size=0.25,\n",
    "    random_state=42,\n",
    "    apply_smote=True,\n",
    "    smote_sampling_strategy='auto',\n",
    "    feature_selection=None,\n",
    "    n_features_to_select=None,\n",
    "    use_cross_validation=False,\n",
    "    cv_folds=5,\n",
    "    save_pipeline=True,\n",
    "    pipeline_path='model/preprocessor.pkl',\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive data preparation for machine learning modeling with optional SMOTE,\n",
    "    feature selection, and cross-validation support.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing all features and target variable\n",
    "    target_column : str, optional (default='left_company')\n",
    "        Name of the target column to predict\n",
    "    categorical_features : list, optional\n",
    "        List of categorical feature column names\n",
    "    numerical_features : list, optional\n",
    "        List of numerical feature column names\n",
    "    test_size : float, optional (default=0.2)\n",
    "        Proportion of the dataset to include in the test split\n",
    "    val_size : float, optional (default=0.25)\n",
    "        Proportion of the training set to include in the validation split\n",
    "    random_state : int, optional (default=42)\n",
    "        Controls the shuffling applied to the data before splitting\n",
    "    apply_smote : bool, optional (default=False)\n",
    "        Whether to apply SMOTE to training data\n",
    "    smote_sampling_strategy : str or float, optional (default='auto')\n",
    "        SMOTE sampling strategy\n",
    "    feature_selection : str, optional (default=None)\n",
    "        Feature selection method: 'selectkbest', 'selectfrommodel', or None\n",
    "    n_features_to_select : int, optional (default=None)\n",
    "        Number of features to select when using feature selection\n",
    "    use_cross_validation : bool, optional (default=False)\n",
    "        Whether to use cross-validation instead of a validation split\n",
    "    cv_folds : int, optional (default=5)\n",
    "        Number of folds for cross-validation\n",
    "    save_pipeline : bool, optional (default=False)\n",
    "        Whether to save the preprocessing pipeline to disk\n",
    "    pipeline_path : str, optional (default='model/preprocessor.pkl')\n",
    "        Path to save the preprocessing pipeline\n",
    "    verbose : bool, optional (default=True)\n",
    "        Whether to print information about the preprocessing steps\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Prepared datasets with additional metadata\n",
    "    \"\"\"\n",
    "    # Default feature lists if not provided\n",
    "    if categorical_features is None:\n",
    "        categorical_features = [\n",
    "            'department', \n",
    "            'salary_level'\n",
    "        ]\n",
    "    \n",
    "    if numerical_features is None:\n",
    "        numerical_features = [\n",
    "            'satisfaction_level', \n",
    "            'last_evaluation', \n",
    "            'num_projects', \n",
    "            'monthly_hours', \n",
    "            'tenure'\n",
    "        ]\n",
    "    \n",
    "    # Step 1: Data quality checks\n",
    "    if verbose:\n",
    "        logger.info(\"Running data quality checks...\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        logger.warning(f\"Missing values detected:\\n{missing_values[missing_values > 0]}\")\n",
    "    \n",
    "    # Check for outliers in numerical features\n",
    "    for col in numerical_features:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]\n",
    "            if len(outliers) > 0:\n",
    "                logger.warning(f\"Outliers detected in {col}: {len(outliers)} values\")\n",
    "    \n",
    "    # Remove redundant columns\n",
    "    def remove_redundant_columns(dataframe, target_col):\n",
    "        \"\"\"Remove columns related to the target variable\"\"\"\n",
    "        columns_to_drop = [\n",
    "            col for col in dataframe.columns \n",
    "            if (col.startswith(target_col) or \n",
    "                col.lower() in ['turnover_status', 'left_status', 'employee_status'])\n",
    "            and col != target_col\n",
    "        ]\n",
    "        return dataframe.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    df_cleaned = remove_redundant_columns(df.copy(), target_column)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df_cleaned.drop(columns=[target_column])\n",
    "    y = df_cleaned[target_column]\n",
    "    \n",
    "    # Step 2: Split the data into train and test sets (and validation if not using CV)\n",
    "    if verbose:\n",
    "        logger.info(\"Splitting data into train/test sets...\")\n",
    "    \n",
    "    # Always create a test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Either create a validation set or prepare for cross-validation\n",
    "    if not use_cross_validation:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size, random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        X_train, y_train = X_temp, y_temp\n",
    "        X_val, y_val = None, None\n",
    "        if verbose:\n",
    "            logger.info(f\"Using {cv_folds}-fold cross-validation instead of validation split\")\n",
    "    \n",
    "    # Step 3: Create preprocessing pipelines\n",
    "    if verbose:\n",
    "        logger.info(\"Creating preprocessing pipeline...\")\n",
    "    \n",
    "    # Initialize the preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "    ]\n",
    "    \n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=preprocessing_steps,\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Initialize feature selection if requested\n",
    "    if feature_selection:\n",
    "        if verbose:\n",
    "            logger.info(f\"Setting up feature selection using {feature_selection}...\")\n",
    "        \n",
    "        if feature_selection == 'selectkbest':\n",
    "            feature_selector = SelectKBest(f_classif, k=n_features_to_select or 'all')\n",
    "        elif feature_selection == 'selectfrommodel':\n",
    "            feature_selector = SelectFromModel(\n",
    "                RandomForestClassifier(random_state=random_state),\n",
    "                max_features=n_features_to_select\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(f\"Unknown feature selection method: {feature_selection}. Using all features.\")\n",
    "            feature_selector = None\n",
    "        \n",
    "        # Create a pipeline with preprocessing and optional feature selection\n",
    "        if feature_selector:\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('feature_selection', feature_selector)\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([('preprocessor', preprocessor)])\n",
    "    else:\n",
    "        pipeline = Pipeline([('preprocessor', preprocessor)])\n",
    "    \n",
    "    # Step 4: Apply preprocessing (and feature selection if enabled)\n",
    "    if verbose:\n",
    "        logger.info(\"Applying preprocessing to data...\")\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    X_train_processed = pipeline.fit_transform(X_train, y_train)\n",
    "    \n",
    "    # Transform validation and test sets\n",
    "    if not use_cross_validation:\n",
    "        X_val_processed = pipeline.transform(X_val)\n",
    "    else:\n",
    "        X_val_processed = None\n",
    "        \n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "    \n",
    "    # Step 5: Apply SMOTE (only to training data and only if not using CV)\n",
    "    if apply_smote and not use_cross_validation:\n",
    "        try:\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "            \n",
    "            if verbose:\n",
    "                logger.info(f\"Applying SMOTE with sampling strategy: {smote_sampling_strategy}\")\n",
    "            \n",
    "            # Store original class distribution for reporting\n",
    "            original_train_len = len(y_train)\n",
    "            original_distribution = pd.Series(y_train).value_counts(normalize=True)\n",
    "            \n",
    "            # Apply SMOTE\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy=smote_sampling_strategy, \n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            X_train_processed, y_train = smote.fit_resample(\n",
    "                X_train_processed, y_train\n",
    "            )\n",
    "            \n",
    "            # Report results if verbose\n",
    "            if verbose:\n",
    "                logger.info(\"\\n--- SMOTE Oversampling Summary ---\")\n",
    "                logger.info(f\"Original training set shape: {original_train_len}\")\n",
    "                logger.info(f\"Original training class distribution:\\n{original_distribution}\")\n",
    "                logger.info(f\"Resampled training set shape: {len(y_train)}\")\n",
    "                logger.info(f\"Resampled class distribution:\\n{pd.Series(y_train).value_counts(normalize=True)}\")\n",
    "        \n",
    "        except ImportError:\n",
    "            logger.warning(\"SMOTE could not be applied. Please install imbalanced-learn (pip install imbalanced-learn)\")\n",
    "    \n",
    "    # Step 6: Set up cross-validation if requested\n",
    "    cv_splits = None\n",
    "    if use_cross_validation:\n",
    "        cv_splits = list(StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state).split(X_train, y_train))\n",
    "        if verbose:\n",
    "            logger.info(f\"Created {len(cv_splits)} cross-validation splits\")\n",
    "    \n",
    "    # Step 7: Get feature names after preprocessing\n",
    "    try:\n",
    "        onehot_encoder = preprocessor.named_transformers_['cat']\n",
    "        cat_feature_names = onehot_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "        \n",
    "        # Get all feature names, including those that weren't transformed\n",
    "        processed_feature_names = (\n",
    "            numerical_features + \n",
    "            cat_feature_names + \n",
    "            [col for col in X_train.columns if col not in numerical_features + categorical_features]\n",
    "        )\n",
    "        \n",
    "        # Adjust feature names if feature selection was applied\n",
    "        if feature_selection and feature_selector and hasattr(feature_selector, 'get_support'):\n",
    "            feature_mask = pipeline.named_steps['feature_selection'].get_support()\n",
    "            processed_feature_names = [name for selected, name in zip(feature_mask, processed_feature_names) if selected]\n",
    "            if verbose:\n",
    "                logger.info(f\"Selected {len(processed_feature_names)} features after feature selection\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error getting feature names: {str(e)}\")\n",
    "        processed_feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "    \n",
    "    # Step 8: Save pipeline if requested\n",
    "    if save_pipeline:\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(pipeline_path), exist_ok=True)\n",
    "            \n",
    "            # Save the pipeline\n",
    "            joblib.dump(pipeline, pipeline_path)\n",
    "            if verbose:\n",
    "                logger.info(f\"Preprocessing pipeline saved to {pipeline_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving pipeline: {str(e)}\")\n",
    "    \n",
    "    # Step 9: Print dataset information if verbose\n",
    "    if verbose:\n",
    "        logger.info(\"\\n--- Data Preparation Summary ---\")\n",
    "        logger.info(f\"Total samples: {len(df)}\")\n",
    "        logger.info(f\"Training set:   {X_train_processed.shape} | Positive class: {y_train.mean():.2%}\")\n",
    "        \n",
    "        if not use_cross_validation:\n",
    "            logger.info(f\"Validation set: {X_val_processed.shape}   | Positive class: {y_val.mean():.2%}\")\n",
    "        else:\n",
    "            logger.info(f\"Using {cv_folds}-fold cross-validation instead of validation split\")\n",
    "            \n",
    "        logger.info(f\"Testing set:    {X_test_processed.shape}  | Positive class: {y_test.mean():.2%}\")\n",
    "        logger.info(f\"Total features after preprocessing: {len(processed_feature_names)}\")\n",
    "    \n",
    "    # Step 10: Return comprehensive dictionary with processed data\n",
    "    result = {\n",
    "        # Original datasets\n",
    "        'X_train_original': X_train,\n",
    "        'X_test_original': X_test,\n",
    "        \n",
    "        # Processed datasets\n",
    "        'X_train_processed': X_train_processed,\n",
    "        'X_test_processed': X_test_processed,\n",
    "        \n",
    "        # Target variables\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        \n",
    "        # Metadata\n",
    "        'pipeline': pipeline,\n",
    "        'feature_names': processed_feature_names,\n",
    "        'categorical_features': categorical_features,\n",
    "        'numerical_features': numerical_features\n",
    "    }\n",
    "    \n",
    "    # Add validation data if not using cross-validation\n",
    "    if not use_cross_validation:\n",
    "        result.update({\n",
    "            'X_val_original': X_val,\n",
    "            'X_val_processed': X_val_processed,\n",
    "            'y_val': y_val\n",
    "        })\n",
    "    # Add cross-validation data if using CV\n",
    "    else:\n",
    "        result.update({\n",
    "            'cv_splits': cv_splits,\n",
    "            'cv_folds': cv_folds\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Application example for HR analytics data\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the enhanced function with appropriate parameters for HR turnover analysis\n",
    "    model_data = prepare_data_for_modeling(\n",
    "        df=hr_data_engineered,  # Your engineered dataset\n",
    "        target_column='left_company',\n",
    "        \n",
    "        # Specify relevant features based on EDA\n",
    "        categorical_features=['department', 'salary_level'],\n",
    "        numerical_features=[\n",
    "            'satisfaction_level', \n",
    "            'last_evaluation', \n",
    "            'num_projects', \n",
    "            'monthly_hours', \n",
    "            'tenure', \n",
    "            'work_intensity',\n",
    "            'poor_work_life_balance',\n",
    "            'burnout_risk',\n",
    "            'high_performer_risk',\n",
    "            'satisfaction_per_hour',\n",
    "            'relative_hours'\n",
    "        ],\n",
    "        \n",
    "        # Split parameters\n",
    "        test_size=0.2,\n",
    "        val_size=0.25,\n",
    "        random_state=42,\n",
    "        \n",
    "        # Enable SMOTE to handle class imbalance\n",
    "        # (since left_company=1 class is only about 16.6% of the data)\n",
    "        apply_smote=True,\n",
    "        smote_sampling_strategy='auto',  # or try 0.8 for slightly less aggressive balancing\n",
    "        \n",
    "        # Enable feature selection\n",
    "        feature_selection='selectfrommodel',  # or 'selectkbest'\n",
    "        n_features_to_select=10,  # Select top 10 most important features\n",
    "        \n",
    "        # Save the pipeline for later use in deployment\n",
    "        save_pipeline=True,\n",
    "        pipeline_path='model/hr_preprocessor.pkl',\n",
    "        \n",
    "        # Enable verbose output\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Access the processed data for modeling\n",
    "    X_train = model_data['X_train_processed']\n",
    "    y_train = model_data['y_train']\n",
    "    X_val = model_data['X_val_processed']\n",
    "    y_val = model_data['y_val']\n",
    "    X_test = model_data['X_test_processed']\n",
    "    y_test = model_data['y_test']\n",
    "\n",
    "    # Access the feature names\n",
    "    feature_names = model_data['feature_names']\n",
    "\n",
    "    # Access the preprocessing pipeline for later use\n",
    "    pipeline = model_data['pipeline']\n",
    "\n",
    "    # Print the selected features\n",
    "    print(\"Selected features for modeling:\")\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        print(f\"{i+1}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, model_name, use_scaled=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model on validation data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The model to train and evaluate\n",
    "    data : dict\n",
    "        Dictionary containing data splits and other metadata\n",
    "    model_name : str\n",
    "        Name of the model for reporting\n",
    "    use_scaled : bool, default=False\n",
    "        Whether to use scaled features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : fitted model\n",
    "        The trained model\n",
    "    y_val_pred : array\n",
    "        Predictions on validation data\n",
    "    y_val_proba : array\n",
    "        Predicted probabilities on validation data\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check which keys are available in the data dictionary\n",
    "    if 'X_train_processed' in data:\n",
    "        # Using the advanced preprocessing function\n",
    "        X_train = data['X_train_processed']\n",
    "        X_val = data['X_val_processed']\n",
    "    elif 'X_train_scaled' in data and use_scaled:\n",
    "        # Using the simpler function with scaling\n",
    "        X_train = data['X_train_scaled']\n",
    "        X_val = data['X_val_scaled']\n",
    "    else:\n",
    "        # Using the simpler function without scaling\n",
    "        X_train = data['X_train']\n",
    "        X_val = data['X_val']\n",
    "    \n",
    "    y_train = data['y_train']\n",
    "    y_val = data['y_val']\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Confusion matrix display\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Stayed', 'Left'])\n",
    "    disp.plot(cmap='Blues', values_format='d', colorbar=False)\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return results\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    return model, y_val_pred, y_val_proba, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Base Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "## 3.3 Base Model Training and Evaluation\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, C=1.0, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_estimators=100, learning_rate=0.1, scale_pos_weight=5)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    fitted_model, val_preds, val_proba, metrics = evaluate_model(model, model_data, name)\n",
    "    results.append(metrics)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison (Validation Set):\")\n",
    "print(comparison_df.set_index('model_name').sort_values('f1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model from initial evaluation\n",
    "best_initial_model = comparison_df.loc[comparison_df['f1'].idxmax(), 'model_name']\n",
    "print(f\"\\nBest initial model based on F1 score: {best_initial_model}\")\n",
    "\n",
    "# Define hyperparameter grid for the best model\n",
    "if best_initial_model == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'min_child_weight': [1, 3]\n",
    "    }\n",
    "    use_scaled = False\n",
    "    model_class = XGBClassifier\n",
    "elif best_initial_model == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    use_scaled = False\n",
    "    model_class = RandomForestClassifier\n",
    "else:  # Logistic Regression\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "    use_scaled = True\n",
    "    model_class = LogisticRegression\n",
    "\n",
    "# First use RandomizedSearchCV to narrow down parameters\n",
    "print(f\"\\nPerforming RandomizedSearchCV for {best_initial_model}...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_class(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on appropriate dataset based on model type\n",
    "X_train = model_data['X_train_processed']  \n",
    "random_search.fit(X_train, model_data['y_train'])\n",
    "print(f\"Best parameters from random search: {random_search.best_params_}\")\n",
    "print(f\"Best F1 score from random search: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Fine-tune with GridSearchCV around best parameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Create refined parameter grid based on initial results\n",
    "fine_tuned_params = {}\n",
    "for param, value in best_params.items():\n",
    "    if param == 'n_estimators':\n",
    "        fine_tuned_params[param] = [max(value-50, 50), value, value+50]\n",
    "    elif param == 'max_depth' and value is not None:\n",
    "        fine_tuned_params[param] = [max(value-1, 1), value, value+1]\n",
    "    elif param == 'learning_rate' and value > 0.01:\n",
    "        fine_tuned_params[param] = [value/2, value, min(value*1.5, 1.0)]\n",
    "    elif param in ['C'] and value > 0.01:\n",
    "        fine_tuned_params[param] = [value/2, value, value*2]\n",
    "    else:\n",
    "        fine_tuned_params[param] = [value]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_class(random_state=42),\n",
    "    param_grid=fine_tuned_params,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, model_data['y_train'])\n",
    "print(f\"Best parameters from grid search: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 score from grid search: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Set best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Find the optimal classification threshold based on F1 score.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True class labels\n",
    "    y_proba : array\n",
    "        Predicted probabilities for positive class\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_threshold : float\n",
    "        The threshold that maximizes the F1 score\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Calculate F1 score for each threshold\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Find the threshold that maximizes the F1 score\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_f1 = f1_scores[optimal_idx]\n",
    "    \n",
    "    # Plot F1 score vs threshold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, f1_scores, '-o', markersize=8)\n",
    "    plt.axvline(x=optimal_threshold, color='red', linestyle='--', \n",
    "                label=f'Optimal Threshold = {optimal_threshold:.2f}')\n",
    "    plt.title('F1 Score vs Classification Threshold', fontsize=16)\n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel('F1 Score', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/threshold_optimization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal threshold: {optimal_threshold:.2f} with F1 score: {optimal_f1:.4f}\")\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# Find optimal threshold\n",
    "print(\"\\nFinding optimal threshold...\")\n",
    "X_val = model_data['X_val_processed']  \n",
    "val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "optimal_threshold = optimize_threshold(model_data['y_val'], val_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Learning Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", cv=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Plot the learning curve for a model to diagnose bias/variance issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "        The model to evaluate\n",
    "    X : array\n",
    "        Training data features\n",
    "    y : array\n",
    "        Training data target\n",
    "    title : str, default=\"Learning Curve\"\n",
    "        Plot title\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    n_jobs : int, default=-1\n",
    "        Number of parallel jobs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate learning curve\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "        train_sizes=train_sizes, scoring='f1')\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Training Examples\", fontsize=14)\n",
    "    plt.ylabel(\"F1 Score\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training and validation scores\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"blue\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"orange\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"blue\",\n",
    "             label=\"Training Score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"orange\",\n",
    "             label=\"Cross-Validation Score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/learning_curve_{title.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curve\n",
    "print(\"\\nPlotting learning curve...\")\n",
    "plot_learning_curve(best_model, model_data['X_train_processed'], model_data['y_train'], \n",
    "                   f\"Learning Curve for {best_initial_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Final Model Evaluation (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_model_evaluation(model, data, use_scaled=False, optimal_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Evaluate the final model on the test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained model\n",
    "    data : dict\n",
    "        Dictionary containing data splits and other metadata\n",
    "    use_scaled : bool, default=False\n",
    "        Whether to use scaled features\n",
    "    optimal_threshold : float, default=0.5\n",
    "        Classification threshold to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    y_test_pred : array\n",
    "        Predictions on test data\n",
    "    y_test_proba : array\n",
    "        Predicted probabilities on test data\n",
    "    \"\"\"\n",
    "    # Select the appropriate test dataset\n",
    "    X_test = data['X_test_processed']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Stayed', 'Left']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Stayed', 'Left'])\n",
    "    disp.plot(cmap='Blues', values_format='d', colorbar=False)\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/final_model_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/final_model_roc_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    return metrics, y_test_pred, y_test_proba\n",
    "\n",
    "# Evaluate final model on test set\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "test_metrics, y_test_pred, y_test_proba = final_model_evaluation(\n",
    "    best_model, model_data, optimal_threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "# Print summary metrics\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {test_metrics['roc_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Calibration Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    Plot calibration curve for a model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True binary labels\n",
    "    y_prob : array\n",
    "        Predicted probabilities\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    \n",
    "    # Plot calibration curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "    plt.plot(prob_pred, prob_true, 's-', label=model_name)\n",
    "    \n",
    "    plt.title(f'Calibration Curve - {model_name}', fontsize=16)\n",
    "    plt.xlabel('Mean Predicted Probability', fontsize=14)\n",
    "    plt.ylabel('Fraction of Positives', fontsize=14)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/calibration_curve_{model_name.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot calibration curve \n",
    "plot_calibration_curve(model_data['y_test'], y_test_proba, best_initial_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, data, feature_names=None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance.\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    # Initialize feature importance DataFrame\n",
    "    feature_importance_df = None\n",
    "    \n",
    "    # Get feature names if not provided\n",
    "    if feature_names is None:\n",
    "        if 'feature_names' in data:\n",
    "            feature_names = data['feature_names']\n",
    "        else:\n",
    "            print(\"Warning: No feature names provided or found in data.\")\n",
    "            feature_names = [f\"Feature {i}\" for i in range(model.n_features_in_)]\n",
    "    \n",
    "    # Try to extract built-in feature importance (if available)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Get feature importance\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Check if lengths match\n",
    "        if len(importances) != len(feature_names):\n",
    "            print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match \"\n",
    "                  f\"feature importances length ({len(importances)})\")\n",
    "            # Use generic feature names if they don't match\n",
    "            feature_names_to_use = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "        else:\n",
    "            feature_names_to_use = feature_names\n",
    "        \n",
    "        # Create DataFrame with feature names and importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names_to_use,\n",
    "            'Importance': importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.barh(feature_importance_df['Feature'][:15], feature_importance_df['Importance'][:15])\n",
    "        plt.title('Feature Importance (Model-Based)', fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "        plt.ylabel('Feature', fontsize=14)\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis to have most important at the top\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/feature_importance.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculate permutation importance (works for any model)\n",
    "    # Select the appropriate dataset\n",
    "    if 'X_test_processed' in data:\n",
    "        X_test = data['X_test_processed']\n",
    "    else:\n",
    "        X_test = data['X_test']\n",
    "    \n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    try:\n",
    "        perm_importance = permutation_importance(model, X_test, y_test, \n",
    "                                               n_repeats=10, random_state=42)\n",
    "        \n",
    "        # Create DataFrame with feature names for permutation importance\n",
    "        # Handle case where feature names don't match X_test dimensions\n",
    "        if len(feature_names) != X_test.shape[1]:\n",
    "            print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match \"\n",
    "                  f\"X_test shape ({X_test.shape[1]})\")\n",
    "            perm_feature_names = [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
    "        else:\n",
    "            perm_feature_names = feature_names\n",
    "        \n",
    "        # Create DataFrame\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            'Feature': perm_feature_names,\n",
    "            'Importance': perm_importance.importances_mean\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Plot permutation importance\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.barh(perm_importance_df['Feature'][:15], perm_importance_df['Importance'][:15])\n",
    "        plt.title('Feature Importance (Permutation-Based)', fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "        plt.ylabel('Feature', fontsize=14)\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis to have most important at the top\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/permutation_importance.png')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating permutation importance: {str(e)}\")\n",
    "        perm_importance_df = pd.DataFrame(columns=['Feature', 'Importance'])\n",
    "    \n",
    "    return feature_importance_df, perm_importance_df\n",
    "\n",
    "# Usage with the enhanced data preparation\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "feature_importance, perm_importance = analyze_feature_importance(\n",
    "    best_model, model_data\n",
    ")\n",
    "\n",
    "# Print top factors\n",
    "if feature_importance is not None:\n",
    "    print(\"\\nTop 10 factors influencing employee turnover (from model):\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 factors from permutation importance (more robust):\")\n",
    "for i, row in perm_importance.head(10).iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Deployment and Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create End-to-End Pipeline for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_end_to_end_pipeline(best_model, model_data, optimal_threshold):\n",
    "    \"\"\"Create and save a complete end-to-end pipeline that combines preprocessing and the model\"\"\"\n",
    "    # Create directory for model artifacts\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "    \n",
    "    # Create a single pipeline that combines preprocessing and model\n",
    "    complete_pipeline = Pipeline([\n",
    "        ('preprocessor', model_data['pipeline']),  # Preprocessing and feature selection\n",
    "        ('model', best_model)                     # The trained model\n",
    "    ])\n",
    "    \n",
    "    # Save the complete pipeline\n",
    "    joblib.dump(complete_pipeline, 'model/complete_pipeline.pkl')\n",
    "    \n",
    "    # Save the threshold separately\n",
    "    with open('model/optimal_threshold.txt', 'w') as f:\n",
    "        f.write(str(optimal_threshold))\n",
    "    \n",
    "    # For reference, save feature names and other metadata\n",
    "    with open('model/model_info.txt', 'w') as f:\n",
    "        f.write(f\"Model type: {type(best_model).__name__}\\n\")\n",
    "        f.write(f\"Optimal threshold: {optimal_threshold}\\n\")\n",
    "        f.write(f\"Features used: {', '.join(model_data['feature_names'])}\\n\")\n",
    "    \n",
    "    print(\"End-to-end pipeline saved successfully\")\n",
    "    return complete_pipeline\n",
    "\n",
    "# Save the complete pipeline\n",
    "create_end_to_end_pipeline(best_model, model_data, optimal_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Engineering Function for New Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features_for_prediction(employee_data):\n",
    "    \"\"\"Apply feature engineering to new employee data\"\"\"\n",
    "    # Convert dict to numeric values where needed\n",
    "    for key, value in employee_data.items():\n",
    "        if key in ['satisfaction_level', 'last_evaluation']:\n",
    "            employee_data[key] = float(value)\n",
    "        elif key in ['num_projects', 'monthly_hours', 'tenure', 'had_accident', 'promotion_last_5yr']:\n",
    "            employee_data[key] = int(value)\n",
    "    \n",
    "    # Calculate work intensity\n",
    "    employee_data['work_intensity'] = employee_data['monthly_hours'] / employee_data['num_projects']\n",
    "    \n",
    "    # Calculate relative hours (assuming 175 is average from our dataset)\n",
    "    avg_hours = 175\n",
    "    employee_data['relative_hours'] = employee_data['monthly_hours'] / avg_hours\n",
    "    \n",
    "    # Calculate overworked\n",
    "    employee_data['overworked'] = int((employee_data['monthly_hours'] > avg_hours) and \n",
    "                                    (employee_data['num_projects'] > 4))\n",
    "    \n",
    "    # Calculate poor work-life balance\n",
    "    employee_data['poor_work_life_balance'] = int(employee_data['monthly_hours'] > 200)\n",
    "\n",
    "    # Calculate productivity\n",
    "    employee_data['productivity'] = employee_data['last_evaluation'] / employee_data['num_projects']\n",
    "    \n",
    "    # Calculate satisfaction per hour\n",
    "    employee_data['satisfaction_per_hour'] = employee_data['satisfaction_level'] / (employee_data['monthly_hours'] / 160)\n",
    "    \n",
    "    # Calculate underutilized\n",
    "    employee_data['underutilized'] = int((employee_data['last_evaluation'] > 0.75) and \n",
    "                                         (employee_data['num_projects'] < 3))\n",
    "    \n",
    "    # Calculate burnout risk\n",
    "    employee_data['burnout_risk'] = int((employee_data['monthly_hours'] > 250) and \n",
    "                                        (employee_data['num_projects'] > 5) and \n",
    "                                        (employee_data['satisfaction_level'] < 0.5))\n",
    "    \n",
    "    # Calculate flight risk\n",
    "    employee_data['flight_risk'] = int((employee_data['satisfaction_level'] < 0.4) and \n",
    "                                       (employee_data['tenure'] > 3) and \n",
    "                                       (employee_data['last_evaluation'] > 0.7))\n",
    "    \n",
    "    # Calculate high performer risk\n",
    "    employee_data['high_performer_risk'] = int((employee_data['last_evaluation'] > 0.8) and \n",
    "                                              (employee_data['promotion_last_5yr'] == 0) and \n",
    "                                              (employee_data['tenure'] > 4))\n",
    "    \n",
    "    return employee_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Risk Identification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_risk_factors(employee_data, probability):\n",
    "    \"\"\"Identify key risk factors for an employee\"\"\"\n",
    "    risk_factors = []\n",
    "    \n",
    "    # Only identify risk factors if probability is concerning\n",
    "    if probability < 0.2:\n",
    "        return [\"No significant risk factors identified.\"]\n",
    "    \n",
    "    # Check various factors based on data analysis\n",
    "    if employee_data['satisfaction_level'] < 0.4:\n",
    "        risk_factors.append(\"Low satisfaction level\")\n",
    "    \n",
    "    if employee_data['last_evaluation'] > 0.8 and employee_data['promotion_last_5yr'] == 0:\n",
    "        risk_factors.append(\"High performer without recent promotion\")\n",
    "    \n",
    "    if employee_data['num_projects'] > 5:\n",
    "        risk_factors.append(\"High workload (too many projects)\")\n",
    "    \n",
    "    if employee_data['monthly_hours'] > 220:\n",
    "        risk_factors.append(\"Excessive working hours\")\n",
    "    \n",
    "    if employee_data['tenure'] > 5 and employee_data['promotion_last_5yr'] == 0:\n",
    "        risk_factors.append(\"Long tenure without promotion\")\n",
    "    \n",
    "    if employee_data['salary_level'] == 'low' and employee_data['last_evaluation'] > 0.7:\n",
    "        risk_factors.append(\"Good performer with low compensation\")\n",
    "    \n",
    "    if employee_data['burnout_risk'] == 1:\n",
    "        risk_factors.append(\"Signs of burnout\")\n",
    "    \n",
    "    if employee_data['work_intensity'] > 50:  # Hours per project is high\n",
    "        risk_factors.append(\"Work is too intense\")\n",
    "        \n",
    "    if employee_data['poor_work_life_balance'] == 1:\n",
    "        risk_factors.append(\"Poor work-life balance\")\n",
    "        \n",
    "    if employee_data['flight_risk'] == 1:\n",
    "        risk_factors.append(\"Classic flight risk profile (low satisfaction but high evaluation)\")\n",
    "    \n",
    "    # If no specific risk factors were identified but probability is high\n",
    "    if len(risk_factors) == 0 and probability > 0.5:\n",
    "        risk_factors.append(\"Multiple moderate factors contributing to turnover risk\")\n",
    "    \n",
    "    return risk_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_actions(risk_factors):\n",
    "    \"\"\"Recommend actions based on identified risk factors\"\"\"\n",
    "    actions = []\n",
    "    \n",
    "    if \"Low satisfaction level\" in risk_factors:\n",
    "        actions.append(\"Schedule a check-in meeting to discuss job satisfaction and address concerns\")\n",
    "    \n",
    "    if \"High performer without recent promotion\" in risk_factors:\n",
    "        actions.append(\"Review promotion timeline and consider career development opportunities\")\n",
    "    \n",
    "    if \"High workload (too many projects)\" in risk_factors:\n",
    "        actions.append(\"Evaluate workload distribution and consider redistributing projects\")\n",
    "    \n",
    "    if \"Excessive working hours\" in risk_factors:\n",
    "        actions.append(\"Monitor work hours and encourage better work-life balance\")\n",
    "    \n",
    "    if \"Long tenure without promotion\" in risk_factors:\n",
    "        actions.append(\"Discuss career path and potential for advancement or skill development\")\n",
    "    \n",
    "    if \"Good performer with low compensation\" in risk_factors:\n",
    "        actions.append(\"Review compensation package in line with performance\")\n",
    "    \n",
    "    if \"Signs of burnout\" in risk_factors or \"Work is too intense\" in risk_factors:\n",
    "        actions.append(\"Provide resources for stress management and consider temporary workload reduction\")\n",
    "    \n",
    "    if \"Poor work-life balance\" in risk_factors:\n",
    "        actions.append(\"Encourage use of vacation time and establish clearer work boundaries\")\n",
    "    \n",
    "    if \"Classic flight risk profile\" in risk_factors:\n",
    "        actions.append(\"Urgent intervention needed - schedule meeting with HR and manager to develop retention plan\")\n",
    "    \n",
    "    # If no specific actions were recommended\n",
    "    if len(actions) == 0:\n",
    "        actions.append(\"General monitoring recommended - no specific intervention needed at this time\")\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Main Prediction Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_employee_turnover(employee_data):\n",
    "    \"\"\"Make a prediction for a single employee with the full pipeline\"\"\"\n",
    "\n",
    "    \n",
    "    # Load the complete pipeline and threshold\n",
    "    try:\n",
    "        complete_pipeline = joblib.load('model/complete_pipeline.pkl')\n",
    "        \n",
    "        with open('model/optimal_threshold.txt', 'r') as f:\n",
    "            optimal_threshold = float(f.read().strip())\n",
    "            \n",
    "        print(\"Models and artifacts loaded successfully\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading model artifacts: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Engineer features for prediction\n",
    "    engineered_data = engineer_features_for_prediction(employee_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    employee_df = pd.DataFrame([engineered_data])\n",
    "    \n",
    "    # Get probability from the complete pipeline\n",
    "    try:\n",
    "        probability = complete_pipeline.predict_proba(employee_df)[0, 1]\n",
    "        prediction = 1 if probability >= optimal_threshold else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Define risk level\n",
    "    if probability < 0.3:\n",
    "        risk_level = \"Low\"\n",
    "    elif probability < 0.7:\n",
    "        risk_level = \"Medium\"\n",
    "    else:\n",
    "        risk_level = \"High\"\n",
    "    \n",
    "    # Identify risk factors and recommend actions\n",
    "    risk_factors = identify_risk_factors(engineered_data, probability)\n",
    "    recommended_actions = recommend_actions(risk_factors)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        \"employee_id\": employee_data.get(\"employee_id\", \"Not provided\"),\n",
    "        \"turnover_prediction\": \"Yes\" if prediction == 1 else \"No\",\n",
    "        \"turnover_probability\": float(probability),\n",
    "        \"risk_level\": risk_level,\n",
    "        \"risk_factors\": risk_factors,\n",
    "        \"recommended_actions\": recommended_actions\n",
    "    }\n",
    "    \n",
    "    # Print the results in a readable format\n",
    "    print(\"\\nEmployee Turnover Risk Assessment\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Employee ID: {result['employee_id']}\")\n",
    "    print(f\"Turnover Prediction: {result['turnover_prediction']}\")\n",
    "    print(f\"Probability of Leaving: {result['turnover_probability']:.1%}\")\n",
    "    print(f\"Risk Level: {result['risk_level']}\")\n",
    "    \n",
    "    print(\"\\nKey Risk Factors:\")\n",
    "    for factor in result['risk_factors']:\n",
    "        print(f\"- {factor}\")\n",
    "    \n",
    "    print(\"\\nRecommended Actions:\")\n",
    "    for action in result['recommended_actions']:\n",
    "        print(f\"- {action}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example employee for prediction\n",
    "    example_employee = {\n",
    "        \"employee_id\": \"EMP12345\",\n",
    "        \"satisfaction_level\": 0.3,\n",
    "        \"last_evaluation\": 0.8,\n",
    "        \"num_projects\": 7,\n",
    "        \"monthly_hours\": 280,\n",
    "        \"tenure\": 4,\n",
    "        \"had_accident\": 0,\n",
    "        \"promotion_last_5yr\": 0,\n",
    "        \"department\": \"sales\", \n",
    "        \"salary_level\": \"medium\"\n",
    "    }\n",
    "    \n",
    "    # Make prediction\n",
    "    result = predict_employee_turnover(example_employee)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
